{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Collaborative filtering\n",
        "This segment explroes more classical methods of recommendations - Collaborative filtering. Surprise, a library built specifically for collaborative filtering is used. This code implements the SVD  (Singular Value Decomposition) algorithm.\n"
      ],
      "metadata": {
        "id": "rL7udBsuqWlE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google colab init and imports"
      ],
      "metadata": {
        "id": "_V9EpY_Sq6rq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall numpy -y\n",
        "!pip install numpy==1.25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "BkytFGWgRf34",
        "outputId": "64c5deb4-7ebd-4a35-a26b-987d42f25a98"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "Collecting numpy==1.25\n",
            "  Downloading numpy-1.25.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Downloading numpy-1.25.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.25.0 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.25.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.25.0 which is incompatible.\n",
            "blosc2 3.3.1 requires numpy>=1.26, but you have numpy 1.25.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.25.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "e4eccf87730644a68afee988461fab0d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install surprise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsmoHHbcB_3v",
        "outputId": "c1a1ca87-2db8-4eae-eae6-616928873b1c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting surprise\n",
            "  Using cached surprise-0.1-py2.py3-none-any.whl.metadata (327 bytes)\n",
            "Collecting scikit-surprise (from surprise)\n",
            "  Using cached scikit_surprise-1.1.4.tar.gz (154 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-surprise->surprise) (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-surprise->surprise) (1.25.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-surprise->surprise) (1.14.1)\n",
            "Downloading surprise-0.1-py2.py3-none-any.whl (1.8 kB)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.4-cp311-cp311-linux_x86_64.whl size=2505212 sha256=f973cf0aace262c2e713c5e6db99a547bbd0d469d2e3c095e4ceff87f97211ff\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/8f/6e/7e2899163e2d85d8266daab4aa1cdabec7a6c56f83c015b5af\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise, surprise\n",
            "Successfully installed scikit-surprise-1.1.4 surprise-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "mP30eERXrk-D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b60c8dd-ba1d-43b2-94c9-e91ea62510e0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "folder_path = '/content/drive/MyDrive/dl project self/GNN_recommender_system-vik_dev'\n",
        "os.chdir(folder_path)\n",
        "print(f\"Current working directory: {os.getcwd()}\")"
      ],
      "metadata": {
        "id": "CrFNaFbIrk8I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6cdc11f-e414-411e-ce01-cf94b0839eb9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: /content/drive/MyDrive/dl project self/GNN_recommender_system-vik_dev\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# loading of data"
      ],
      "metadata": {
        "id": "Bopcywkpvcac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "from surprise import Dataset, Reader, SVD\n",
        "from surprise import accuracy\n",
        "from collections import defaultdict\n",
        "import time"
      ],
      "metadata": {
        "id": "WPXlSOAwvYbO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edge_cols = [\"user_id\", \"parent_asin\", \"rating\"]\n",
        "rating_scale = (1, 5)\n",
        "model_filename = 'best_svd_model_tuned_train_test.joblib'\n",
        "k_for_recall = 10"
      ],
      "metadata": {
        "id": "ONKejWAh30Kg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_split = \"train_test\"\n",
        "data_dir = 'data'\n",
        "\n",
        "if user_split == \"train_test_valid\":\n",
        "  train_df = pd.read_parquet(f\"{data_dir}/{user_split}_split/train.parquet\", columns = edge_cols)\n",
        "  test_df = pd.read_parquet(f\"{data_dir}/{user_split}_split/valid.parquet\", columns = edge_cols)\n",
        "else:\n",
        "  train_df = pd.read_parquet(f\"{data_dir}/{user_split}_split/train.parquet\", columns = edge_cols)\n",
        "  test_df = pd.read_parquet(f\"{data_dir}/{user_split}_split/test.parquet\", columns = edge_cols)"
      ],
      "metadata": {
        "id": "3lKG--YCxcAY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# loading data into model"
      ],
      "metadata": {
        "id": "NBXUIEDsGbSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nPreparing training data for Surprise library...\")\n",
        "reader = Reader(rating_scale=rating_scale)\n",
        "\n",
        "# training data\n",
        "data_for_training = Dataset.load_from_df(train_df[['user_id', 'parent_asin', 'rating']], reader)\n",
        "print(\"Training data successfully loaded into Surprise format.\")\n",
        "trainset = data_for_training.build_full_trainset()\n",
        "print(\"Surprise trainset created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjVKTjfpybBY",
        "outputId": "98f5c29c-73a5-4b06-c939-51fb5a7ebc89"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Preparing training data for Surprise library...\n",
            "Training data successfully loaded into Surprise format.\n",
            "Surprise trainset created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# init SVD model\n",
        "This model was trained using params found in train, test and valid split fine tuning."
      ],
      "metadata": {
        "id": "rSZoXrViGliS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train SVD model\n",
        "print(\"\\nTraining the SVD model using fixed parameters on the training data...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# instantiate SVD with the fixed parameters\n",
        "svd_model = SVD(\n",
        "    n_factors=50,\n",
        "    n_epochs=20,\n",
        "    lr_all=0.01,\n",
        "    reg_all=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "# train the model on the training set\n",
        "svd_model.fit(trainset)\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"SVD model trained successfully in {end_time - start_time:.2f} seconds.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rj4xgx6Dya_x",
        "outputId": "ce513cf8-c087-4df3-8118-d060e0ecb385"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training the SVD model using fixed parameters on the training data...\n",
            "SVD model trained successfully in 7.40 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# saving of model"
      ],
      "metadata": {
        "id": "KYkImvHsG5ta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# saving model\n",
        "print(f\"\\nSaving the trained model to {model_filename}...\")\n",
        "joblib.dump(svd_model, model_filename)\n",
        "print(\"Model saved.\")"
      ],
      "metadata": {
        "id": "mfNSp4XMya7u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f4756fe-2b3a-4d55-984f-60820388bcd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saving the trained model to best_svd_model_tuned_train_test.joblib...\n",
            "Model saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading of model"
      ],
      "metadata": {
        "id": "oyURvKBpCV_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load model from file\n",
        "print(f\"\\nLoading the model from {model_filename} for evaluation...\")\n",
        "loaded_model = joblib.load(model_filename)\n",
        "print(\"Model loaded successfully (or using in-memory model).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Jcz5fxxCW-Y",
        "outputId": "c52ba1f5-8172-4b1a-c513-31e422179e15"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading the model from best_svd_model_tuned_train_test.joblib for evaluation...\n",
            "Model loaded successfully (or using in-memory model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# eval of recall@10"
      ],
      "metadata": {
        "id": "HaenTSd3G7rw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# show global mean\n",
        "global_mean_rating = loaded_model.trainset.global_mean\n",
        "print(f\"Global mean rating from training data: {global_mean_rating:.4f}\")\n",
        "\n",
        "print(\"\\nPreparing test data for Recall@K calculation...\")\n",
        "test_users = test_df['user_id'].unique()\n",
        "items = list(test_df['parent_asin'].unique()) + list(train_df['parent_asin'].unique())\n",
        "print(f\"Found {len(test_users)} unique users and {len(items)} unique items.\")\n",
        "\n",
        "# define groudn truth\n",
        "test_ground_truth = test_df.groupby('user_id')['parent_asin'] \\\n",
        "                           .apply(set) \\\n",
        "                           .to_dict()\n",
        "\n",
        "print(f\"Ground truth created for {len(test_ground_truth)} users in the test set.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oR_4O3bJmus",
        "outputId": "dbfa6444-9921-4f05-9b05-29903160cae5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global mean rating from training data: 3.9933\n",
            "\n",
            "Preparing test data for Recall@K calculation...\n",
            "Found 96760 unique users and 101700 unique items.\n",
            "Ground truth created for 96760 users in the test set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate top k pred in test set\n",
        "print(f\"\\nGenerating Top-{k_for_recall} predictions for each user in the test set...\")\n",
        "start_time = time.time()\n",
        "top_n_predictions_test = defaultdict(list)\n",
        "\n",
        "for user_id in test_users:\n",
        "    user_predictions = []\n",
        "\n",
        "    for item_id in items:\n",
        "        # surprise handles users/items not seen in training data by returning the global average rating.\n",
        "        prediction = loaded_model.predict(uid=user_id, iid=item_id)\n",
        "        user_predictions.append((item_id, prediction.est))\n",
        "\n",
        "    # sort pred by highest rated above\n",
        "    user_predictions.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # get the top 10 item ID\n",
        "    top_k_items = [iid for iid, est in user_predictions[:k_for_recall]]\n",
        "    top_n_predictions_test[user_id] = top_k_items\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Top-{k_for_recall} prediction generation for test set complete in {end_time - start_time:.2f} seconds.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4Qg5r064iJW",
        "outputId": "52f7e4db-59f2-4220-8a02-d062fc68d023"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating Top-10 predictions for each user in the test set...\n",
            "Top-10 prediction generation for test set complete in 18592.52 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Calculate recall@K on testset\n",
        "print(f\"\\nCalculating Recall@{k_for_recall} on the test set...\")\n",
        "user_recalls_test = []\n",
        "\n",
        "for user_id, relevant_items in test_ground_truth.items():\n",
        "    predicted_top_k = top_n_predictions_test.get(user_id, []) # Use .get for safety\n",
        "    predicted_set = set(predicted_top_k)\n",
        "\n",
        "    # find the number of hits\n",
        "    hits = len(relevant_items.intersection(predicted_set))\n",
        "\n",
        "    # find recall for this user\n",
        "    if len(relevant_items) > 0:\n",
        "        recall = hits / len(relevant_items)\n",
        "        user_recalls_test.append(recall)\n",
        "    else:\n",
        "        user_recalls_test.append(0.0)\n",
        "\n",
        "# calculate the average recall across all users evaluated in the test set\n",
        "if user_recalls_test:\n",
        "    average_recall_at_k_test = sum(user_recalls_test) / len(user_recalls_test)\n",
        "else:\n",
        "    average_recall_at_k_test = 0.0 # to avoid division by zero\n",
        "\n",
        "print(f\"\\n--- Final Evaluation Results (Test Set with Fixed Params) ---\")\n",
        "print(f\"Average Recall@{k_for_recall}: {average_recall_at_k_test:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Xx-wfgh4iHF",
        "outputId": "82c00556-d424-483e-9156-79b8b4931c39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculating Recall@10 on the test set...\n",
            "\n",
            "--- Final Evaluation Results (Test Set with Fixed Params) ---\n",
            "Average Recall@10: 0.0004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing out variety of products\n",
        "By predicting on the first 5 test users, we want to test if the variety of products suggested is wide."
      ],
      "metadata": {
        "id": "EFmfl2nnC81-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import itertools # Used for efficient pair iteration\n",
        "\n",
        "def generate_recommendations_and_calculate_overlaps(loaded_model, users_to_predict, all_items, k):\n",
        "    print(f\"--- Starting Recommendation Generation (Top-{k}) ---\")\n",
        "    top_n_predictions = defaultdict(list)\n",
        "    num_users = len(list(users_to_predict))\n",
        "\n",
        "    # --- 1. Generate Top-K Predictions ---\n",
        "    for i, user_id in enumerate(users_to_predict):\n",
        "        if (i + 1) % 50 == 0 or i == num_users - 1:\n",
        "             print(f\"Processing user {i+1}/{num_users} ({user_id})...\")\n",
        "\n",
        "        user_predictions = []\n",
        "        for item_id in all_items:\n",
        "          prediction = loaded_model.predict(uid=user_id, iid=item_id)\n",
        "          user_predictions.append((item_id, prediction.est))\n",
        "        user_predictions.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        top_k_items = [iid for iid, est in user_predictions[:k]]\n",
        "        top_n_predictions[user_id] = top_k_items\n",
        "\n",
        "    print(\"--- Recommendation Generation Complete ---\")\n",
        "    print(f\"Generated predictions for {len(top_n_predictions)} users.\")\n",
        "\n",
        "    # overlaps\n",
        "    print(\"--- Starting Overlap Calculation ---\")\n",
        "    overlaps = {}\n",
        "    user_ids_with_preds = list(top_n_predictions.keys())\n",
        "\n",
        "    if len(user_ids_with_preds) < 2:\n",
        "        print(\"Need at least two users with predictions to calculate overlaps.\")\n",
        "        return overlaps, top_n_predictions # Return empty overlaps\n",
        "\n",
        "    total_pairs = len(user_ids_with_preds) * (len(user_ids_with_preds) - 1) // 2\n",
        "    processed_pairs = 0\n",
        "\n",
        "    for user1_id, user2_id in itertools.combinations(user_ids_with_preds, 2):\n",
        "        set1 = set(top_n_predictions[user1_id])\n",
        "        set2 = set(top_n_predictions[user2_id])\n",
        "\n",
        "        # calculate the intersection\n",
        "        overlap_count = len(set1.intersection(set2))\n",
        "        overlaps[(user1_id, user2_id)] = overlap_count\n",
        "\n",
        "        processed_pairs += 1\n",
        "        if processed_pairs % 1000 == 0 or processed_pairs == total_pairs: # Print progress\n",
        "            print(f\"Calculating overlaps: Processed {processed_pairs}/{total_pairs} pairs...\")\n",
        "\n",
        "\n",
        "    print(\"--- Overlap Calculation Complete ---\")\n",
        "    return overlaps, top_n_predictions"
      ],
      "metadata": {
        "id": "Fu6-mJiWvYQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users_found_in_trainset = ['AGKHLEW2SOWHNMFQIJGBECAF7INQ', 'AGKHLEW2SOWHNMFQIJGBECAF7INQ', 'AGMJ3EMDVL6OWBJF7CA5RGJLXN5A']\n",
        "generate_recommendations_and_calculate_overlaps(loaded_model, users_found_in_trainset, items, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0XT7qxhl6o4",
        "outputId": "bcb6b4f2-c1fc-4289-95eb-a7d3a3f84d78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Recommendation Generation (Top-10) ---\n",
            "Processing user 3/3 (AGMJ3EMDVL6OWBJF7CA5RGJLXN5A)...\n",
            "--- Recommendation Generation Complete ---\n",
            "Generated predictions for 2 users.\n",
            "--- Starting Overlap Calculation ---\n",
            "Calculating overlaps: Processed 1/1 pairs...\n",
            "--- Overlap Calculation Complete ---\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({('AGKHLEW2SOWHNMFQIJGBECAF7INQ', 'AGMJ3EMDVL6OWBJF7CA5RGJLXN5A'): 1},\n",
              " defaultdict(list,\n",
              "             {'AGKHLEW2SOWHNMFQIJGBECAF7INQ': ['B08CKCV9HD',\n",
              "               'B07DL991L4',\n",
              "               'B07NN4VC8Z',\n",
              "               'B07T9Z9P82',\n",
              "               'B07G5R9BKW',\n",
              "               'B078W2K47L',\n",
              "               'B07FFG6TGS',\n",
              "               'B005OSAI78',\n",
              "               'B078W2K47L',\n",
              "               'B07G5R9BKW'],\n",
              "              'AGMJ3EMDVL6OWBJF7CA5RGJLXN5A': ['B089FQDTPS',\n",
              "               'B089FQDTPS',\n",
              "               'B00481CYIS',\n",
              "               'B00LG03KNM',\n",
              "               'B00QJ7TVMG',\n",
              "               'B008COCMKM',\n",
              "               'B002T5NMGS',\n",
              "               'B082FLYSY5',\n",
              "               'B082FLYSY5',\n",
              "               'B07G5R9BKW']}))"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Good variety in product recommended such that barely any overlaps in products reccomended"
      ],
      "metadata": {
        "id": "aglTFpUtmaW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "users_not_found_in_trainset = ['AGF42GID7QWDCNFTJRCTMKAITJJA', 'AHZ6XMOLEWA67S3TX7IWEXXGWSOA', 'AE5DIA2HDWBPNGBO2FXN2PF4NQJA']\n",
        "generate_recommendations_and_calculate_overlaps(loaded_model, users_not_found_in_trainset, items, 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdS7GrqClZxr",
        "outputId": "a71aec29-f296-444c-e245-bdbd99abe896"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Recommendation Generation (Top-4) ---\n",
            "Processing user 3/3 (AE5DIA2HDWBPNGBO2FXN2PF4NQJA)...\n",
            "--- Recommendation Generation Complete ---\n",
            "Generated predictions for 3 users.\n",
            "--- Starting Overlap Calculation ---\n",
            "Calculating overlaps: Processed 3/3 pairs...\n",
            "--- Overlap Calculation Complete ---\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({('AGF42GID7QWDCNFTJRCTMKAITJJA', 'AHZ6XMOLEWA67S3TX7IWEXXGWSOA'): 4,\n",
              "  ('AGF42GID7QWDCNFTJRCTMKAITJJA', 'AE5DIA2HDWBPNGBO2FXN2PF4NQJA'): 4,\n",
              "  ('AHZ6XMOLEWA67S3TX7IWEXXGWSOA', 'AE5DIA2HDWBPNGBO2FXN2PF4NQJA'): 4},\n",
              " defaultdict(list,\n",
              "             {'AGF42GID7QWDCNFTJRCTMKAITJJA': ['B002T5NMGS',\n",
              "               'B01F0RV4G6',\n",
              "               'B00H3WGN9K',\n",
              "               'B00DD6I2GM'],\n",
              "              'AHZ6XMOLEWA67S3TX7IWEXXGWSOA': ['B002T5NMGS',\n",
              "               'B01F0RV4G6',\n",
              "               'B00H3WGN9K',\n",
              "               'B00DD6I2GM'],\n",
              "              'AE5DIA2HDWBPNGBO2FXN2PF4NQJA': ['B002T5NMGS',\n",
              "               'B01F0RV4G6',\n",
              "               'B00H3WGN9K',\n",
              "               'B00DD6I2GM']}))"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "More overlaps for users with no purchase history as they will generally get recommended the same things"
      ],
      "metadata": {
        "id": "cdBCxVC_mk3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NDCG@10"
      ],
      "metadata": {
        "id": "6zV57cz2WKhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dcg_at_k(scores, k):\n",
        "    \"\"\"\n",
        "    Calculates Discounted Cumulative Gain @ k.\n",
        "    Args:\n",
        "        scores (list): List of relevance scores (e.g., [1.0, 0.0, 1.0, ...]).\n",
        "        k (int): The cutoff point.\n",
        "    Returns:\n",
        "        float: The DCG@k value.\n",
        "    \"\"\"\n",
        "    # Convert scores to a tensor, considering only the top k\n",
        "    scores_tensor = torch.tensor(scores[:k], dtype=torch.float32)\n",
        "    if scores_tensor.numel() == 0:\n",
        "        return 0.0\n",
        "    # Create ranks tensor starting from 1\n",
        "    ranks = torch.arange(1, scores_tensor.numel() + 1, dtype=torch.float32)\n",
        "    # Calculate discounts using log base 2\n",
        "    discounts = torch.log2(ranks + 1)\n",
        "    # Compute DCG\n",
        "    return torch.sum(scores_tensor / discounts).item()\n",
        "\n",
        "def ndcg_at_k(true_items_set, predicted_items_list, k):\n",
        "    \"\"\"\n",
        "    Calculates Normalized Discounted Cumulative Gain @ k.\n",
        "    Args:\n",
        "        true_items_set (set): The set of relevant item IDs (e.g., parent_asin) for a user.\n",
        "        predicted_items_list (list): The ordered list of predicted item IDs (e.g., parent_asin).\n",
        "        k (int): The cutoff point.\n",
        "    Returns:\n",
        "        float: The NDCG@k value.\n",
        "    \"\"\"\n",
        "    # Handle empty predictions\n",
        "    if not predicted_items_list:\n",
        "        return 0.0\n",
        "\n",
        "    # Determine relevance scores for the top k predicted items\n",
        "    # Relevance is 1.0 if the predicted item is in the true set, else 0.0\n",
        "    relevance_scores = [1.0 if item in true_items_set else 0.0 for item in predicted_items_list[:k]]\n",
        "\n",
        "    # Calculate DCG for the actual predicted list @ k\n",
        "    actual_dcg = dcg_at_k(relevance_scores, k)\n",
        "\n",
        "    # Calculate Ideal DCG (IDCG) @ k\n",
        "    # The ideal list contains all true items ranked first (up to k)\n",
        "    num_true_items = len(true_items_set)\n",
        "    # Ideal scores are 1.0 for each relevant item, capped by k\n",
        "    ideal_scores = [1.0] * min(k, num_true_items)\n",
        "    ideal_dcg = dcg_at_k(ideal_scores, k)\n",
        "\n",
        "    # Calculate NDCG, handle division by zero if IDCG is 0\n",
        "    if ideal_dcg == 0:\n",
        "        return 0.0 # No relevant items means perfect score is 0, or cannot normalize\n",
        "    else:\n",
        "        return actual_dcg / ideal_dcg"
      ],
      "metadata": {
        "id": "c_IXeuZHl1_0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Ground Truth using the TEST data: All items interacted with by each user in the test set\n",
        "test_ground_truth = test_df.groupby('user_id')['parent_asin'] \\\n",
        " .apply(set) \\\n",
        " .to_dict()\n",
        "\n",
        "print(f\"Ground truth created for {len(test_ground_truth)} users in the test set.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Or7yFfsBWsAl",
        "outputId": "1bcfacf7-2ec3-40da-8460-2f699e6b9913"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground truth created for 96760 users in the test set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all unique users from the test set\n",
        "all_test_users = test_df['user_id'].unique()\n",
        "\n",
        "# Select only the first 1000 users\n",
        "subset_users = all_test_users[:1000]\n",
        "\n",
        "# unique items from both train and test sets\n",
        "items = list(set(list(test_df['parent_asin'].unique()) + list(train_df['parent_asin'].unique())))\n",
        "print(f\"Total unique items to predict from: {len(items)}\")\n",
        "\n",
        "print(f\"\\nGenerating Top-{k_for_recall} predictions for the first {len(subset_users)} users in the test set...\")\n",
        "start_time = time.time()\n",
        "top_n_predictions_test = defaultdict(list)\n",
        "\n",
        "for user_id in subset_users:\n",
        "    user_predictions = []\n",
        "    for item_id in items:\n",
        "          prediction = loaded_model.predict(uid=user_id, iid=item_id)\n",
        "          user_predictions.append((item_id, prediction.est))\n",
        "\n",
        "    # sort predictions\n",
        "    user_predictions.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # top K item IDs\n",
        "    top_k_items = [iid for iid, est in user_predictions[:k_for_recall]]\n",
        "    top_n_predictions_test[user_id] = top_k_items\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Top-{k_for_recall} prediction generation for {len(subset_users)} users.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5v1BAILIWr-U",
        "outputId": "c804a153-e8d1-4f1f-cee9-807378e64dbb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique items to predict from: 89060\n",
            "\n",
            "Generating Top-10 predictions for the first 1000 users in the test set...\n",
            "Top-10 prediction generation for 1000 users.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "k = k_for_recall\n",
        "users_for_ndcg_eval = list(top_n_predictions_test.keys())\n",
        "\n",
        "print(f\"\\nCalculating NDCG@{k} for the {len(users_for_ndcg_eval)} users with generated predictions...\")\n",
        "user_ndcgs_subset = []\n",
        "start_time_ndcg = time.time()\n",
        "\n",
        "for user_id in users_for_ndcg_eval:\n",
        "    true_items = test_ground_truth.get(user_id, set())\n",
        "    predicted_top_k = top_n_predictions_test.get(user_id, [])\n",
        "    user_ndcg = ndcg_at_k(true_items, predicted_top_k, k)\n",
        "    user_ndcgs_subset.append(user_ndcg)\n",
        "\n",
        "end_time_ndcg = time.time()\n",
        "\n",
        "# calculate the average NDCG@k for this subset\n",
        "if user_ndcgs_subset:\n",
        "    average_ndcg_at_k_subset = sum(user_ndcgs_subset) / len(user_ndcgs_subset)\n",
        "else:\n",
        "    average_ndcg_at_k_subset = 0.0\n",
        "    print(\"Warning: No NDCG scores calculated. Check if the selected users have ground truth data.\")\n",
        "\n",
        "\n",
        "print(f\"NDCG@{k} calculation for the subset of {len(users_for_ndcg_eval)} users complete in {end_time_ndcg - start_time_ndcg:.2f} seconds.\")\n",
        "print(f\"Average NDCG@{k} (Collaborative Filtering - First {len(users_for_ndcg_eval)} Users): {average_ndcg_at_k_subset:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GczRO9WoWr8B",
        "outputId": "d43d9c7d-0de3-4b03-8f91-bac32a6e17ad"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculating NDCG@10 for the 1000 users with generated predictions...\n",
            "NDCG@10 calculation for the subset of 1000 users complete in 0.27 seconds.\n",
            "Average NDCG@10 (Collaborative Filtering - First 1000 Users): 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ROGPmMx5DCGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HX5gQaDcEbMM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}