{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Collaborative filtering\n",
        "This segment explroes more classical methods of recommendations - Collaborative filtering. Surprise, a library built specifically for collaborative filtering is used. This code implements the SVD  (Singular Value Decomposition) algorithm.\n"
      ],
      "metadata": {
        "id": "rL7udBsuqWlE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google colab init and imports"
      ],
      "metadata": {
        "id": "_V9EpY_Sq6rq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall numpy -y\n",
        "!pip install numpy==1.25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "BkytFGWgRf34",
        "outputId": "50a57450-dfd7-40f7-c363-19aa12bf8acc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "Collecting numpy==1.25\n",
            "  Downloading numpy-1.25.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Downloading numpy-1.25.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.25.0 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.25.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.25.0 which is incompatible.\n",
            "blosc2 3.3.1 requires numpy>=1.26, but you have numpy 1.25.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.25.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "079d822ec5134c33be56618f6a9b056f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install surprise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Blj0I2Xs-vVw",
        "outputId": "1ab7b30c-3123-4bf8-c567-8060eba9b916"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting surprise\n",
            "  Downloading surprise-0.1-py2.py3-none-any.whl.metadata (327 bytes)\n",
            "Collecting scikit-surprise (from surprise)\n",
            "  Downloading scikit_surprise-1.1.4.tar.gz (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-surprise->surprise) (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-surprise->surprise) (1.25.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-surprise->surprise) (1.14.1)\n",
            "Downloading surprise-0.1-py2.py3-none-any.whl (1.8 kB)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.4-cp311-cp311-linux_x86_64.whl size=2505220 sha256=c89e9eaad02d3f2046b850473f38559a06e6d88ad27876e0878a930c3a4c94b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/8f/6e/7e2899163e2d85d8266daab4aa1cdabec7a6c56f83c015b5af\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise, surprise\n",
            "Successfully installed scikit-surprise-1.1.4 surprise-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mP30eERXrk-D",
        "outputId": "533d6c79-5388-4dac-ec21-882f7ad5aad2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "folder_path = '/content/drive/MyDrive/dl project self/GNN_recommender_system-vik_dev'\n",
        "os.chdir(folder_path)\n",
        "print(f\"Current working directory: {os.getcwd()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrFNaFbIrk8I",
        "outputId": "365f97f4-4feb-47e7-f804-0c8c2ff72506"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: /content/drive/MyDrive/dl project self/GNN_recommender_system-vik_dev\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# loading of data"
      ],
      "metadata": {
        "id": "KnEygQd3rC-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "from surprise import Dataset, Reader, SVD\n",
        "from surprise.model_selection import GridSearchCV\n",
        "from surprise import accuracy\n",
        "from collections import defaultdict\n",
        "import time"
      ],
      "metadata": {
        "id": "WPXlSOAwvYbO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_split = \"train_test_valid\"\n",
        "data_dir = 'data'\n",
        "\n",
        "edge_cols = [\"user_id\", \"parent_asin\", \"rating\"]\n",
        "rating_scale = (1, 5)\n",
        "model_filename = 'best_svd_model.joblib'\n",
        "k_for_recall = 10\n",
        "\n",
        "if user_split == \"train_test_valid\":\n",
        "  train_df = pd.read_parquet(f\"{data_dir}/{user_split}_split/train.parquet\", columns = edge_cols)\n",
        "  test_df = pd.read_parquet(f\"{data_dir}/{user_split}_split/valid.parquet\", columns = edge_cols)\n",
        "else:\n",
        "  train_df = pd.read_parquet(f\"{data_dir}/{user_split}_split/train.parquet\", columns = edge_cols)\n",
        "  test_df = pd.read_parquet(f\"{data_dir}/{user_split}_split/test.parquet\", columns = edge_cols)"
      ],
      "metadata": {
        "id": "3lKG--YCxcAY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# gridsearch params"
      ],
      "metadata": {
        "id": "2PeR8UvzLOGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'n_factors': [50, 100],\n",
        "    'n_epochs': [20, 30],\n",
        "    'lr_all': [0.005, 0.01],\n",
        "    'reg_all': [0.02, 0.1]\n",
        "}\n",
        "print(f\"Parameter grid for GridSearchCV: {param_grid}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13l8fha5vYW1",
        "outputId": "109f1608-50a9-4d6d-c63d-4752034f5207"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter grid for GridSearchCV: {'n_factors': [50, 100], 'n_epochs': [20, 30], 'lr_all': [0.005, 0.01], 'reg_all': [0.02, 0.1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# loading data into model"
      ],
      "metadata": {
        "id": "_zQB2rJyLUoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nPreparing training data for Surprise library...\")\n",
        "reader = Reader(rating_scale=rating_scale)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDRbM9p8vYST",
        "outputId": "9b25b7d7-217f-4065-ae1b-dd9021c402f5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Preparing training data for Surprise library...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_for_tuning = Dataset.load_from_df(train_df[['user_id', 'parent_asin', 'rating']], reader)\n",
        "print(\"Training data successfully loaded into Surprise format for tuning.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjVKTjfpybBY",
        "outputId": "f9baf885-3ff2-4e44-eb2a-76a19b708b8a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data successfully loaded into Surprise format for tuning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nStarting hyperparameter tuning with GridSearchCV on the training data...\")\n",
        "start_time = time.time()\n",
        "\n",
        "gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3, n_jobs=-1)\n",
        "gs.fit(data_for_tuning)\n",
        "\n",
        "end_time = time.time()\n",
        "print(\"\\n--- Grid Search Results ---\")\n",
        "print(f\"Tuning completed in {end_time - start_time:.2f} seconds.\")\n",
        "print(\"Best RMSE score on training data (cross-validation): {:.4f}\".format(gs.best_score['rmse']))\n",
        "print(\"Best parameters found for RMSE: {}\".format(gs.best_params['rmse']))\n",
        "print(\"---------------------------\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rj4xgx6Dya_x",
        "outputId": "80ba5ba1-6799-41b8-fec3-fe3a0853a8b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting hyperparameter tuning with GridSearchCV on the training data...\n",
            "\n",
            "--- Grid Search Results ---\n",
            "Tuning completed in 259.52 seconds.\n",
            "Best RMSE score on training data (cross-validation): 1.3636\n",
            "Best parameters found for RMSE: {'n_factors': 50, 'n_epochs': 20, 'lr_all': 0.01, 'reg_all': 0.1}\n",
            "---------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# training final model"
      ],
      "metadata": {
        "id": "P1toaB7lLrLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training the final SVD model with the best parameters found by GridSearchCV...\")\n",
        "print(f\"Using parameters: {gs.best_params['rmse']}\")\n",
        "start_time = time.time()\n",
        "\n",
        "best_params = gs.best_params['rmse']\n",
        "\n",
        "final_model = SVD(\n",
        "    n_factors=gs.best_params['rmse']['n_factors'],\n",
        "    n_epochs=gs.best_params['rmse']['n_epochs'],\n",
        "    lr_all=gs.best_params['rmse']['lr_all'],\n",
        "    reg_all=gs.best_params['rmse']['reg_all'],\n",
        "    random_state=42 # for reproducibility\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfNSp4XMya7u",
        "outputId": "d6e00e1d-024a-418a-a3cb-fd70e0a9eb4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the final SVD model with the best parameters found by GridSearchCV...\n",
            "Using parameters: {'n_factors': 50, 'n_epochs': 20, 'lr_all': 0.01, 'reg_all': 0.1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_model.fit(data_for_tuning)\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Final model trained successfully in {end_time - start_time:.2f} seconds.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QtHrGTnya5S",
        "outputId": "82f5a223-b909-4659-dff2-c2b35314634e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final model trained successfully in 156.62 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving model"
      ],
      "metadata": {
        "id": "NVy3laep-pEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nSaving the final tuned model to {model_filename}...\")\n",
        "joblib.dump(final_model, model_filename)\n",
        "print(\"Model saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFA63ScA2NMp",
        "outputId": "c0c70c11-83aa-4330-a123-f12a48737187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saving the final tuned model to svd_model_train_only.joblib...\n",
            "Model saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading model"
      ],
      "metadata": {
        "id": "PmMTlGsQ-qxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_final_model = joblib.load(model_filename)\n",
        "print(\"Model loaded successfully (or using in-memory final_model).\")\n",
        "\n",
        "global_mean_rating = loaded_final_model.trainset.global_mean\n",
        "print(f\"Global mean rating from training data: {global_mean_rating:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncqiVW2F2VT8",
        "outputId": "3a0416ee-553f-4bed-f8ea-1175b559b041"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully (or using in-memory final_model).\n",
            "Global mean rating from training data: 3.9563\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# recall@10"
      ],
      "metadata": {
        "id": "tqDewdD6NnQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nPreparing test data for Recall@K calculation...\")\n",
        "\n",
        "test_users = test_df['user_id'].unique()\n",
        "items = list(test_df['parent_asin'].unique()) + list(train_df['parent_asin'].unique())\n",
        "print(f\"Found {len(test_users)} unique users and {len(test_items)} unique items\")\n",
        "\n",
        "# Define Ground Truth using the TEST data: All items interacted with by each user in the test set\n",
        "test_ground_truth = test_df.groupby('user_id')['parent_asin'] \\\n",
        "                           .apply(set) \\\n",
        "                           .to_dict()\n",
        "\n",
        "print(f\"Ground truth created for {len(test_ground_truth)} users in the test set.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1PIYX5YNyRE",
        "outputId": "ff5fe441-0d0f-49b3-f6df-cf8e8a4b9395"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Preparing test data for Recall@K calculation...\n",
            "Found 97950 unique users and 28578 unique items\n",
            "Ground truth created for 97950 users in the test set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nGenerating Top-{k_for_recall} predictions for each user in the test set using the final tuned model...\")\n",
        "start_time = time.time()\n",
        "top_n_predictions_test = defaultdict(list)\n",
        "\n",
        "for user_id in test_users:\n",
        "    user_predictions = []\n",
        "\n",
        "    for item_id in test_items:\n",
        "        prediction = loaded_final_model.predict(uid=user_id, iid=item_id)\n",
        "        user_predictions.append((item_id, prediction.est))\n",
        "\n",
        "    user_predictions.sort(key=lambda x: x[1], reverse=True)\n",
        "    top_k_items = [iid for iid, est in user_predictions[:k_for_recall]]\n",
        "    top_n_predictions_test[user_id] = top_k_items\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Top-{k_for_recall} prediction generation for test set complete in {end_time - start_time:.2f} seconds.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-J_pykg2ieJ",
        "outputId": "76a2d710-10f1-4ab9-af2e-8592a34b6fd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating Top-10 predictions for each user in the test set using the final tuned model...\n",
            "Top-10 prediction generation for test set complete in 17473.94 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nCalculating Recall@{k_for_recall} on the test set...\")\n",
        "user_recalls_test = []\n",
        "\n",
        "for user_id, relevant_items in test_ground_truth.items():\n",
        "    predicted_top_k = top_n_predictions_test.get(user_id, [])\n",
        "    predicted_set = set(predicted_top_k)\n",
        "\n",
        "    hits = len(relevant_items.intersection(predicted_set))\n",
        "    if len(relevant_items) > 0:\n",
        "        recall = hits / len(relevant_items)\n",
        "        user_recalls_test.append(recall)\n",
        "    else:\n",
        "        user_recalls_test.append(0.0)\n",
        "\n",
        "if user_recalls_test:\n",
        "    average_recall_at_k_test = sum(user_recalls_test) / len(user_recalls_test)\n",
        "else:\n",
        "    average_recall_at_k_test = 0.0\n",
        "\n",
        "print(f\"\\n--- Final Evaluation Results (Test Set after GridSearchCV) ---\")\n",
        "print(f\"Average Recall@{k_for_recall}: {average_recall_at_k_test:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quVQJqtUya3N",
        "outputId": "9c5469de-8a0e-4b1c-d674-e72727e30234"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculating Recall@10 on the test set...\n",
            "\n",
            "--- Final Evaluation Results (Test Set after GridSearchCV) ---\n",
            "Average Recall@10: 0.0032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NDCG@10\n",
        "Run for first 1000 people due to limited computational resource"
      ],
      "metadata": {
        "id": "F4nlllQDn232"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dcg_at_k(scores, k):\n",
        "    \"\"\"\n",
        "    Calculates Discounted Cumulative Gain @ k.\n",
        "    Args:\n",
        "        scores (list): List of relevance scores (e.g., [1.0, 0.0, 1.0, ...]).\n",
        "        k (int): The cutoff point.\n",
        "    Returns:\n",
        "        float: The DCG@k value.\n",
        "    Code from ChatGPT\n",
        "    \"\"\"\n",
        "    # Convert scores to a tensor, considering only the top k\n",
        "    scores_tensor = torch.tensor(scores[:k], dtype=torch.float32)\n",
        "    if scores_tensor.numel() == 0:\n",
        "        return 0.0\n",
        "    # Create ranks tensor starting from 1\n",
        "    ranks = torch.arange(1, scores_tensor.numel() + 1, dtype=torch.float32)\n",
        "    # Calculate discounts using log base 2\n",
        "    discounts = torch.log2(ranks + 1)\n",
        "    # Compute DCG\n",
        "    return torch.sum(scores_tensor / discounts).item()\n",
        "\n",
        "def ndcg_at_k(true_items_set, predicted_items_list, k):\n",
        "    \"\"\"\n",
        "    Calculates Normalized Discounted Cumulative Gain @ k.\n",
        "    Args:\n",
        "        true_items_set (set): The set of relevant item IDs (e.g., parent_asin) for a user.\n",
        "        predicted_items_list (list): The ordered list of predicted item IDs (e.g., parent_asin).\n",
        "        k (int): The cutoff point.\n",
        "    Returns:\n",
        "        float: The NDCG@k value.\n",
        "    \"\"\"\n",
        "    # Handle empty predictions\n",
        "    if not predicted_items_list:\n",
        "        return 0.0\n",
        "\n",
        "    # Determine relevance scores for the top k predicted items\n",
        "    # Relevance is 1.0 if the predicted item is in the true set, else 0.0\n",
        "    relevance_scores = [1.0 if item in true_items_set else 0.0 for item in predicted_items_list[:k]]\n",
        "\n",
        "    # Calculate DCG for the actual predicted list @ k\n",
        "    actual_dcg = dcg_at_k(relevance_scores, k)\n",
        "\n",
        "    # Calculate Ideal DCG (IDCG) @ k\n",
        "    # The ideal list contains all true items ranked first (up to k)\n",
        "    num_true_items = len(true_items_set)\n",
        "    # Ideal scores are 1.0 for each relevant item, capped by k\n",
        "    ideal_scores = [1.0] * min(k, num_true_items)\n",
        "    ideal_dcg = dcg_at_k(ideal_scores, k)\n",
        "\n",
        "    # Calculate NDCG, handle division by zero if IDCG is 0\n",
        "    if ideal_dcg == 0:\n",
        "        return 0.0 # No relevant items means perfect score is 0, or cannot normalize\n",
        "    else:\n",
        "        return actual_dcg / ideal_dcg"
      ],
      "metadata": {
        "id": "607n7uLYVrxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Ground Truth using the TEST data: All items interacted with by each user in the test set\n",
        "test_ground_truth = test_df.groupby('user_id')['parent_asin'] \\\n",
        " .apply(set) \\\n",
        " .to_dict()\n",
        "\n",
        "print(f\"Ground truth created for {len(test_ground_truth)} users in the test set.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__4CUOgXsV4q",
        "outputId": "2f3883db-1b51-4f24-9564-d068ccd7670a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground truth created for 97950 users in the test set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all unique users from the test set\n",
        "all_test_users = test_df['user_id'].unique()\n",
        "\n",
        "# Select only the first 1000 users\n",
        "subset_users = all_test_users[:1000]\n",
        "\n",
        "# unique items from both train and test sets\n",
        "items = list(set(list(test_df['parent_asin'].unique()) + list(train_df['parent_asin'].unique())))\n",
        "print(f\"Total unique items to predict from: {len(items)}\")\n",
        "\n",
        "print(f\"\\nGenerating Top-{k_for_recall} predictions for the first {len(subset_users)} users in the test set...\")\n",
        "start_time = time.time()\n",
        "top_n_predictions_test = defaultdict(list)\n",
        "\n",
        "for user_id in subset_users:\n",
        "    user_predictions = []\n",
        "    for item_id in items:\n",
        "          prediction = loaded_final_model.predict(uid=user_id, iid=item_id)\n",
        "          user_predictions.append((item_id, prediction.est))\n",
        "\n",
        "    # sort predictions\n",
        "    user_predictions.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # top K item IDs\n",
        "    top_k_items = [iid for iid, est in user_predictions[:k_for_recall]]\n",
        "    top_n_predictions_test[user_id] = top_k_items\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Top-{k_for_recall} prediction generation for {len(subset_users)} users.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e04Omz2yStbX",
        "outputId": "f0b5fea2-3d52-407d-dbfa-6b2948c0529c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique items to predict from: 70033\n",
            "\n",
            "Generating Top-10 predictions for the first 1000 users in the test set...\n",
            "Top-10 prediction generation for 1000 users.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "k = k_for_recall\n",
        "users_for_ndcg_eval = list(top_n_predictions_test.keys())\n",
        "\n",
        "print(f\"\\nCalculating NDCG@{k} for the {len(users_for_ndcg_eval)} users with generated predictions...\")\n",
        "user_ndcgs_subset = []\n",
        "start_time_ndcg = time.time()\n",
        "\n",
        "for user_id in users_for_ndcg_eval:\n",
        "    true_items = test_ground_truth.get(user_id, set())\n",
        "    predicted_top_k = top_n_predictions_test.get(user_id, [])\n",
        "    user_ndcg = ndcg_at_k(true_items, predicted_top_k, k)\n",
        "    user_ndcgs_subset.append(user_ndcg)\n",
        "\n",
        "end_time_ndcg = time.time()\n",
        "\n",
        "# calculate the average NDCG@k for this subset\n",
        "if user_ndcgs_subset:\n",
        "    average_ndcg_at_k_subset = sum(user_ndcgs_subset) / len(user_ndcgs_subset)\n",
        "else:\n",
        "    average_ndcg_at_k_subset = 0.0\n",
        "    print(\"Warning: No NDCG scores calculated. Check if the selected users have ground truth data.\")\n",
        "\n",
        "\n",
        "print(f\"NDCG@{k} calculation for the subset of {len(users_for_ndcg_eval)} users complete in {end_time_ndcg - start_time_ndcg:.2f} seconds.\")\n",
        "print(f\"Average NDCG@{k} (Collaborative Filtering - First {len(users_for_ndcg_eval)} Users): {average_ndcg_at_k_subset:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeTpzcYdStZA",
        "outputId": "7de5901a-ff87-4e5b-d61e-81d86c148d25"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculating NDCG@10 for the 1000 users with generated predictions...\n",
            "NDCG@10 calculation for the subset of 1000 users complete in 0.09 seconds.\n",
            "Average NDCG@10 (Collaborative Filtering - First 1000 Users): 0.0005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KCn9bX9Kn4qF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MxHUBzyqn4n_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "39pQ7RFOn4l2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5EZds7WZn4kC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}