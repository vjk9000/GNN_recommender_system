{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#Embedding methods - summary"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e21d48760e61e8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Chunk & aggregate\n",
    "  - Split text into smaller chunks\n",
    "    - Sentences? Paras? Topics?\n",
    "    - No. of tokens? (Fixed size chunking)\n",
    "  - Embed each indiv chunk.\n",
    "  - Aggregate chunk embeddings\n",
    "    - Average\n",
    "    - Maxpool\n",
    "    - Attention mechanisms\n",
    "2. Hierarchical embeddings\n",
    "  - Embed small units of text\n",
    "  - Combine embeddings to reflect position and relationships (encode relationships)\n",
    "  - Potentially capture overall summary/sentiment of the review at highest level\n",
    "  - [Hierarchical Embedding for Amazon personalized product search](https://github.com/QingyaoAi/Hierarchical-Embedding-Model-for-Personalized-Product-Search)\n",
    "  - Can also consider for product metadata to encode product features?\n",
    "3. Extend context windows\n",
    "  - Positional encoding interpolation/extrapolation (sinusoidal)\n",
    "  - Attention approximation\n",
    "4. Existing embedding models for consideration?\n",
    "Comparison: BLaIR (roberta-base is 768)\n",
    "\n",
    "  **3.1 Standalone models**\n",
    "\n",
    "  Types to consider:\n",
    "\n",
    " A) larger embedding dimensions, more robust but increases training complexity\n",
    " - OpenAI ada-002 (but costs [$0.10 per 1M tokens](https://platform.openai.com/docs/pricing) ): embedding dim 1024\n",
    "\n",
    " B) smaller dim, less complex\n",
    " - E5-small-v2 (embedding dim 384, multilingual, but also truncates at 512 tokens)\n",
    "\n",
    " C) Embedding model + some other layers eg [BERT-BiGRU](https://link.springer.com/article/10.1007/s44196-025-00747-1#Tab1)\n",
    " - BiGRU: bidirectional gated recurrent unit.\n",
    " - Generate word embeddings from BERT first, then extract key features using BiGRU\n",
    " - research paper later feeds output features into classification GCN\n",
    "\n",
    " Other more custom options:\n",
    "\n",
    " D) other simpler/traditional methods eg glove, word2vec etc\n",
    "\n",
    " E) autoencoders?\n",
    "\n",
    "  **3.2 Embedding model integrated w GNN**\n",
    "  - BERTGCN: combines BERT and GNN arch"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1b3c6b80cd032b4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3dd909cdb0372112"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "folder_path = '/content/drive/My Drive/DL 28 project_copy/Amazon review GNN + BLaIR'\n",
    "os.chdir(folder_path)\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "product_df = pd.read_csv('./Pre-processed data/All_Beauty cleaned data/product_metadata.csv')\n",
    "product_df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d67e14998bf2aecf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fixed size chunking"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e181c62d5ca4d288"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "TOKEN_LENGTH_PER_CHUNK = 512\n",
    "\n",
    "class FixedSizeChunker():\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        self.model = AutoModel.from_pretrained(MODEL_NAME).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def fixed_size_chunking(self, review):\n",
    "        tokenized = self.tokenizer(review, padding=False, truncation=False, return_tensors=\"pt\")\n",
    "        input_ids_full = tokenized['input_ids'].squeeze(0).to(self.device)\n",
    "        attn_mask_full = tokenized['attention_mask'].squeeze(0).to(self.device)\n",
    "\n",
    "        input_id_chunks = list(input_ids_full.split(TOKEN_LENGTH_PER_CHUNK - 2))\n",
    "        mask_chunks = list(attn_mask_full.split(TOKEN_LENGTH_PER_CHUNK - 2))\n",
    "\n",
    "        # for chunk in input_id_chunks:\n",
    "        # print(len(chunk))\n",
    "\n",
    "        for i in range(len(input_id_chunks)):\n",
    "            input_id_chunks[i] = torch.cat(\n",
    "                (torch.Tensor([101]).to(self.device), input_id_chunks[i], torch.Tensor([102]).to(self.device)))\n",
    "            mask_chunks[i] = torch.cat(\n",
    "                [torch.Tensor([1]).to(self.device), mask_chunks[i], torch.Tensor([1]).to(self.device)])\n",
    "\n",
    "            req_pad_len = TOKEN_LENGTH_PER_CHUNK - input_id_chunks[i].shape[0]\n",
    "\n",
    "            if req_pad_len > 0:\n",
    "                input_id_chunks[i] = torch.nn.functional.pad(input_id_chunks[i], (0, req_pad_len),\n",
    "                                                             value=self.tokenizer.pad_token_id)\n",
    "                mask_chunks[i] = torch.nn.functional.pad(mask_chunks[i], (0, req_pad_len), value=0)\n",
    "\n",
    "        return torch.stack(input_id_chunks).long(), torch.stack(mask_chunks)\n",
    "\n",
    "    def aggregate_embeddings(self, input_id_chunks, attn_mask_chunks, method='mean'):\n",
    "        output = self.model(input_id_chunks, attn_mask_chunks)\n",
    "        if method == 'mean':\n",
    "            return output.last_hidden_state.mean(dim=0)\n",
    "            # check again\n",
    "        if method == 'maxpool':\n",
    "            return output.last_hidden_state.max(dim=0)\n",
    "\n",
    "    def chunk_and_embed(self, review, method='mean'):\n",
    "        input_chunks, mask = self.fixed_size_chunking(review)\n",
    "        print(f'number of chunks: {input_chunks.shape[0]}')\n",
    "        input_chunks = input_chunks.to(self.device)\n",
    "        mask = mask.to(self.device)\n",
    "        embeddings = self.aggregate_embeddings(input_chunks, mask, method)\n",
    "        return embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5cb8830003badc66"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "chunker = FixedSizeChunker()\n",
    "\n",
    "sample_product_df = product_df.iloc[0:5].copy()\n",
    "sample_product_df.loc[:, 'embedding_fixed_chunk'] = sample_product_df['reviews'].apply(lambda review: chunker.chunk_and_embed(review))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "878b1d0abe21dc04"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'shape of embedding: {sample_product_df.embedding_fixed_chunk.iloc[0].shape}')\n",
    "sample_product_df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cccba5b331ade017"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
